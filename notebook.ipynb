{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "import uuid\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(directory_path, tokenizer, chunk_size, para_seperator=\" /n /n\", separator=\" \"):\n",
    "\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    documents = {}\n",
    "    all_chunks = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        print(filename)\n",
    "        base = os.path.basename(file_path)\n",
    "        sku = os.path.splitext(base)[0]\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            doc_id = str(uuid.uuid4())    \n",
    "\n",
    "            paragraphs = re.split(para_seperator, text)\n",
    "\n",
    "            for paragraph in paragraphs:\n",
    "                words = paragraph.split(separator)\n",
    "                current_chunk_str = \"\"\n",
    "                chunk = []\n",
    "                for word in words:\n",
    "                    if current_chunk_str:\n",
    "                        new_chunk = current_chunk_str + separator + word\n",
    "                    else:\n",
    "                        new_chunk = current_chunk_str + word    \n",
    "                    if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
    "                        current_chunk_str = new_chunk\n",
    "                    else:\n",
    "                        if current_chunk_str:\n",
    "                            chunk.append(current_chunk_str)\n",
    "                        current_chunk_str = word\n",
    "                \n",
    "\n",
    "                if current_chunk_str:   \n",
    "                    chunk.append(current_chunk_str)\n",
    "\n",
    "                for chunk in chunk:\n",
    "                    chunk_id = str(uuid.uuid4())\n",
    "                    all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"file_name\":sku}}\n",
    "        documents[doc_id] = all_chunks\n",
    "    return documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_document_embeddings(documents, tokenizer, model):\n",
    "    mapped_document_db = {}\n",
    "    for id, dict_content in documents.items():\n",
    "        mapped_embeddings = {}\n",
    "        for content_id, text_content in dict_content.items():\n",
    "            text = text_content.get(\"text\")\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "            mapped_embeddings[content_id] = embeddings\n",
    "        mapped_document_db[id] = mapped_embeddings\n",
    "    return mapped_document_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_information(query, top_k, mapped_document_db):\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings=query_embeddings.tolist()\n",
    "    #converting query embeddings to numpy array\n",
    "    query_embeddings=np.array(query_embeddings)\n",
    "\n",
    "    scores = {}\n",
    "    #Now calculating cosine similarity\n",
    "    for doc_id, chunk_dict in mapped_document_db.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "            #converting chunk embedding to numpy array for efficent mathmetical operations\n",
    "            chunk_embeddings = np.array(chunk_embeddings) \n",
    "\n",
    "            #Normalizing chunk embeddings and query embeddings  to get cosine similarity score\n",
    "            normalized_query = np.linalg.norm(query_embeddings)\n",
    "            normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "\n",
    "            if normalized_chunk == 0 or normalized_query == 0:\n",
    "            # this is being done to avoid division with zero which will give wrong results i.e infinity. Hence to avoid this we set score to 0\n",
    "                score == 0\n",
    "            else:\n",
    "            # Now calculationg cosine similarity score\n",
    "                score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)  \n",
    "\n",
    "             #STORING SCORES WITH THE REFERENCE\n",
    "            scores[(doc_id, chunk_id )] = score   \n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)\n",
    "    return top_results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(query, tokenizer, model):\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings=query_embeddings.tolist()\n",
    "    return query_embeddings\n",
    "\n",
    "def calculate_cosine_similarity_score(query_embeddings, chunk_embeddings):\n",
    "        normalized_query = np.linalg.norm(query_embeddings)\n",
    "        normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "        if normalized_chunk == 0 or normalized_query == 0:\n",
    "            score == 0\n",
    "        else:\n",
    "            score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)  \n",
    "        return score    \n",
    "\n",
    "\n",
    "def retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k):\n",
    "    scores = {}\n",
    "\n",
    "    for doc_id, chunk_dict in mapped_document_db.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "        #converting chunk embedding to numpy array for efficent mathmetical operations\n",
    "            chunk_embeddings = np.array(chunk_embeddings) \n",
    "            score = calculate_cosine_similarity_score(query_embeddings, chunk_embeddings)\n",
    "            scores[(doc_id, chunk_id )] = score \n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "\n",
    "    return sorted_scores        \n",
    "\n",
    "def retrieve_top_results(sorted_scores):\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)\n",
    "    return top_results    \n",
    "\n",
    "def save_json(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def retrieve_text(top_results, document_data):\n",
    "    first_match = top_results[1]\n",
    "    doc_id = first_match[0]\n",
    "    chunk_id = first_match[1]\n",
    "    related_text = document_data[doc_id][chunk_id]\n",
    "    return related_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_response(model, tokenizer, query, relavent_text):\n",
    "    input_text = f\"\"\"\n",
    "    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n",
    "\n",
    "    Your job is to understand the request, and answer based on the retrieved context.\n",
    "    Here is context:\n",
    "    {relavent_text} \n",
    "    \n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    print(len(inputs['input_ids'][0]))\n",
    "    outputs = model.generate(**inputs, max_length=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"documents\"\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "chunk_size = 200\n",
    "para_seperator=\" /n /n\"\n",
    "separator=\" \"\n",
    "top_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating document store with chunk id, doc_id, text\n",
    "documents = chunking(directory_path, tokenizer, chunk_size, para_seperator, separator)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now embedding generation and mapping in database\n",
    "mapped_document_db = map_document_embeddings(documents, tokenizer, model)\n",
    "mapped_document_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving json\n",
    "save_json('database/doc_store_2.json', documents) \n",
    "save_json('database/vector_store_2.json', mapped_document_db)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"google/flan-t5-base\"  # Replace with your chosen model\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in whatever situation you’re in. Concentrate on putting your plan into action when the tantrum happens.\n",
      "    • Accept that you can’t control your child’s emotions or behaviour directly. You can only keep your child safe and guide their behaviour so tantrums are less likely to happen in the future.\n",
      "    • Accept that it takes time for change to happen. Your child has a lot of growing up to do before tantrums are gone forever. Developing and practising self-regulation skills is a life-long task.\n",
      "    • Beware of thinking that your child is doing it on purpose or trying to upset you. Children don’t have tantrums deliberately. They’re stuck in a bad habit or don’t have the skills right now to cope with the situation.\n",
      "    • Keep your sense of humour. But don’t laugh at the tantrum – if you do, it might reward your child with attention. It might also upset your child even more if they think\n",
      "\n",
      "    You are an intelligent search engine. You will be provided with some retrieved context, and users query.\n",
      "    Here is context:\n",
      "    in whatever situation you’re in. Concentrate on putting your plan into action when the tantrum happens.\n",
      "    • Accept that you can’t control your child’s emotions or behaviour directly. You can only keep your child safe and guide their behaviour so tantrums are less likely to happen in the future.\n",
      "    • Accept that it takes time for change to happen. Your child has a lot of growing up to do before tantrums are gone forever. Developing and practising self-regulation skills is a life-long task.\n",
      "    • Beware of thinking that your child is doing it on purpose or trying to upset you. Children don’t have tantrums deliberately. They’re stuck in a bad habit or don’t have the skills right now to cope with the situation.\n",
      "    • Keep your sense of humour. But don’t laugh at the tantrum – if you do, it might reward your child with attention. It might also upset your child even more if they think \n",
      "    Question: How to make tantrums less likely to happen?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#Retrieving most relavent data chunks\n",
    "query = \"How to make tantrums less likely to happen?\"\n",
    "query_embeddings = compute_embeddings(query, tokenizer, model)\n",
    "sorted_scores = retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k)\n",
    "top_results = retrieve_top_results(sorted_scores)\n",
    "#print(top_results)\n",
    "\n",
    "#reading json\n",
    "document_data = read_json(\"database/doc_store_2.json\") #read document store\n",
    "\n",
    "#Retrieving text of relavent chunk embeddings\n",
    "relavent_text = retrieve_text(top_results, document_data)\n",
    "\n",
    "#print(relavent_text)\n",
    "print(relavent_text[\"text\"])\n",
    "\n",
    "input_text = f\"\"\"\n",
    "    You are an intelligent search engine. You will be provided with some retrieved context, and users query.\n",
    "    Here is context:\n",
    "    {relavent_text['text']} \n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'in whatever situation you’re in. Concentrate on putting your plan into action when the tantrum happens.\\n    • Accept that you can’t control your child’s emotions or behaviour directly. You can only keep your child safe and guide their behaviour so tantrums are less likely to happen in the future.\\n    • Accept that it takes time for change to happen. Your child has a lot of growing up to do before tantrums are gone forever. Developing and practising self-regulation skills is a life-long task.\\n    • Beware of thinking that your child is doing it on purpose or trying to upset you. Children don’t have tantrums deliberately. They’re stuck in a bad habit or don’t have the skills right now to cope with the situation.\\n    • Keep your sense of humour. But don’t laugh at the tantrum – if you do, it might reward your child with attention. It might also upset your child even more if they think',\n",
       " 'metadata': {'file_name': 'behaviuor2'}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_text(top_results, document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "response = generate_llm_response(llm_model, llm_tokenizer, query, relavent_text[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accept that you can’t control your child’s emotions or behaviour directly. You can only keep your child safe and guide their behaviour so tantrums are less likely to happen in the future'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
